{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Saliency_and_visualization_3Supports_tfdotkeras.ipynb","provenance":[{"file_id":"1J1hLG5l2gkDWJL9B_6G0ESjtHc0tI-Nu","timestamp":1590940874465},{"file_id":"1zDrg0xGkrxtH9Bb-JZfqtsow3YhfpAOQ","timestamp":1590718648884},{"file_id":"1UOTb-3taYZbAjo-lZ-il9e6lmZS-Z6DY","timestamp":1589660285680},{"file_id":"https://github.com/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part3.ipynb","timestamp":1589347586105}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jTEzoMx6CasV"},"source":["#### Copyright 2018 Google LLC."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YHK6DyunSbs4"},"source":["## Classification of Various Metal Oxide Materials with Model Fine-Tuning\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dI5rmt4UBwXs"},"source":["## Feature Extraction Using a Pretrained Model\n","\n","One thing that is commonly done in computer vision is to take a model trained on a very large dataset, run it on your own, smaller dataset, and extract the intermediate representations (features) that the model generates. These representations are frequently informative for your own computer vision task, even though the task may be quite different from the problem that the original model was trained on. This versatility and repurposability of convnets is one of the most interesting aspects of deep learning.\n","\n","In our case, we will use the [Inception V3 model](https://arxiv.org/abs/1512.00567) developed at Google, and pre-trained on [ImageNet](http://image-net.org/), a large dataset of web images (1.4M images and 1000 classes). This is a powerful model; let's see what the features that it has learned can do for our cat vs. dog problem.\n","\n","First, we need to pick which intermediate layer of Inception V3 we will use for feature extraction. A common practice is to use the output of the very last layer before the `Flatten` operation, the so-called \"bottleneck layer.\" The reasoning here is that the following fully connected layers will be too specialized for the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n","\n","Let's instantiate an Inception V3 model preloaded with weights trained on ImageNet:\n"]},{"cell_type":"code","metadata":{"id":"xloUbdoBguFA","colab_type":"code","outputId":"760a58fb-d0a1-48f3-b123-ad3944a5cedd","executionInfo":{"status":"ok","timestamp":1591250424276,"user_tz":420,"elapsed":470,"user":{"displayName":"Emmett Goodman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZgry0mLX_raoX3Aoy_pgRSsNgya_tL4VNHSuqTA=s64","userId":"08545279306958699650"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":[" #mount google drive with images\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1xJZ5glPPCRz","outputId":"c1951ee0-f19d-4108-fb56-07faca37ab66","executionInfo":{"status":"ok","timestamp":1591250427363,"user_tz":420,"elapsed":3534,"user":{"displayName":"Emmett Goodman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZgry0mLX_raoX3Aoy_pgRSsNgya_tL4VNHSuqTA=s64","userId":"08545279306958699650"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pip install -q pyyaml h5py  # Required to save models in HDF5 format\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.optimizers import RMSprop\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","%tensorflow_version 2.x\n","print('TensorFlow version is {}'.format(tf.__version__))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["TensorFlow version is 2.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xQRn5grz3PF5","colab_type":"code","outputId":"b3262014-187f-43bb-e703-f12ba2014ecd","executionInfo":{"status":"ok","timestamp":1591250428522,"user_tz":420,"elapsed":4670,"user":{"displayName":"Emmett Goodman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZgry0mLX_raoX3Aoy_pgRSsNgya_tL4VNHSuqTA=s64","userId":"08545279306958699650"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["#check that you have the right GPUs and RAM\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n","Thu Jun  4 06:00:27 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    35W / 250W |    353MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Your runtime has 27.4 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VaXLMtYiF0t9"},"source":["Now let's download the weights:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KMrbllgAFipZ","colab":{}},"source":["#https://www.tensorflow.org/tutorials/keras/\n","#https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/custom_callback.ipynb#scrollTo=Ct0VCSI2dt3a\n","#above reference is related to callbacks and modularity of training\n","def create_model():\n","    !wget --no-check-certificate \\\n","        https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n","        -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\n","    #from tensorflow.keras.applications.inception_v3 import InceptionV3\n","    #from tensorflow.keras.optimizers import RMSprop\n","\n","    local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n","    pre_trained_model = InceptionV3(\n","        input_shape=(150, 150, 3), include_top=False, weights=None)\n","    pre_trained_model.load_weights(local_weights_file)\n","\n","    #lets make the model non-trainable, since we will only use it for feature extraction\n","    #we wont update the weights of the pretrained model during training\n","    for layer in pre_trained_model.layers:\n","      layer.trainable = False\n","\n","    #The layer we will use for feature extraction in Inception v3 is called mixed7. \n","    #It is not the bottleneck of the network, but we are using it to keep a sufficiently \n","    #large feature map (7x7 in this case). (Using the bottleneck layer would have \n","    #resulting in a 3x3 feature map, which is a bit small.) Let's get the output from mixed7:\n","    last_layer = pre_trained_model.get_layer('mixed7')\n","    last_output = last_layer.output\n","\n","    #Now let's stick a fully connected classifier on top of last_output:\n","    # Flatten the output layer to 1 dimension\n","    x = layers.Flatten()(last_output)\n","    # Add a fully connected layer with 1,024 hidden units and ReLU activation\n","    x = layers.Dense(1024, activation='relu')(x)\n","    # Add a dropout rate of 0.2\n","    x = layers.Dropout(0.2)(x)\n","    # Add a final softmax layer for classification\n","    x = layers.Dense(3, activation='softmax', name='visualized_layer')(x)\n","\n","    # Configure and compile the model\n","    model = Model(pre_trained_model.input, x)\n","    \n","    # Return pre-compiled model\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b675yYBTjZnW","colab_type":"code","colab":{}},"source":["#add data locations\n","#this is class balanced with only thre classes\n","train_dir = './drive/My Drive/SupportClassification/three_supports/train'\n","validation_dir = './drive/My Drive/SupportClassification/three_supports/validation'\n","\n","train_NPs_dir = os.path.join(train_dir, 'NPs')\n","train_SiO2NBs_dir = os.path.join(train_dir, 'SiO2NBs')\n","train_TiO2_dir = os.path.join(train_dir, 'TiO2')\n","\n","validation_NPs_dir = os.path.join(validation_dir, 'NPs')\n","validation_SiO2NBs_dir = os.path.join(validation_dir, 'SiO2NBs')\n","validation_TiO2_dir = os.path.join(validation_dir, 'TiO2')\n","\n","num_NPs_tr = len(os.listdir(train_NPs_dir))\n","num_SiO2NBs_tr = len(os.listdir(train_SiO2NBs_dir))\n","num_TiO2_tr = len(os.listdir(train_TiO2_dir))\n","\n","num_NPs_val = len(os.listdir(validation_NPs_dir))\n","num_SiO2NBs_val = len(os.listdir(validation_SiO2NBs_dir))\n","num_TiO2_val = len(os.listdir(validation_TiO2_dir))\n","\n","total_train = num_NPs_tr + num_TiO2_tr\n","\n","total_val = num_NPs_val + num_TiO2_val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6svGYXVwhdOk","colab_type":"code","colab":{}},"source":["#Lets define our callback function, starting with our save directory\n","checkpoint_path = \"drive/My Drive/SupportClassification/three_supports/SandV3_modelweights/weights.{epoch:02d}-{val_acc:.2f}.hdf5.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTP6JXUJ-FdI","colab_type":"text"},"source":["Set up the generators to generate sets of training and test data."]},{"cell_type":"code","metadata":{"id":"PXtMcowgv5a9","colab_type":"code","outputId":"807d0e3c-b117-4db7-9ee0-5d9640fb300c","executionInfo":{"status":"ok","timestamp":1591250514396,"user_tz":420,"elapsed":90463,"user":{"displayName":"Emmett Goodman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZgry0mLX_raoX3Aoy_pgRSsNgya_tL4VNHSuqTA=s64","userId":"08545279306958699650"}},"colab":{"base_uri":"https://localhost:8080/","height":683}},"source":["epochs = [5]\n","zoom_ranges = [0.5]\n","width_shift_ranges = [.5]\n","height_shift_ranges = [.5]\n","learning_rates = [.0001]\n","\n","for epoch in epochs:\n","  for zr in zoom_ranges:\n","    for wsr in width_shift_ranges:\n","      for hsr in height_shift_ranges:\n","        for lr in learning_rates:\n","\n","          print('Optimizing with epoch {}, zoom_range {}, width_shift_range {}, and height_shift_range {} and learning rate {}'.format(epoch, zr, wsr, hsr, lr))\n","\n","          model = create_model()\n","          print('Extracted pre_trained_model!')\n","          \n","          model.compile(loss='binary_crossentropy',\n","                optimizer=RMSprop(lr),\n","                metrics=['acc'])\n","\n","          train_image_generator = ImageDataGenerator(\n","                              rescale=1./255,\n","                              #rotation_range=45,\n","                              #width_shift_range = wsr,\n","                              #height_shift_range = hsr,\n","                              horizontal_flip=True,\n","                              #zoom_range = zr\n","                              )\n","          \n","          validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n","\n","          batch_size = 128\n","          epochs = epoch\n","          IMG_HEIGHT = 150\n","          IMG_WIDTH = 150\n","\n","          train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n","                                                                    directory=train_dir,\n","                                                                    shuffle=True,\n","                                                                    target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                                    class_mode='categorical')\n","\n","          val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n","                                                                        directory=validation_dir,\n","                                                                        target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                                        class_mode='categorical',\n","                                                                        shuffle = False)\n","          \n","####################################GET#CALLBACKS#READY####################################                   \n","          history = model.fit_generator(\n","              train_data_gen,\n","              steps_per_epoch=total_train // batch_size,\n","              epochs=epochs,\n","              validation_data=val_data_gen,\n","              validation_steps=total_val // batch_size,\n","              callbacks=[cp_callback]  \n","              )"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Optimizing with epoch 5, zoom_range 0.5, width_shift_range 0.5, and height_shift_range 0.5 and learning rate 0.0001\n","--2020-06-04 06:00:29--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.128, 2607:f8b0:400c:c13::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 87910968 (84M) [application/x-hdf]\n","Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n","\n","/tmp/inception_v3_w 100%[===================>]  83.84M  93.8MB/s    in 0.9s    \n","\n","2020-06-04 06:00:30 (93.8 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n","\n","Extracted pre_trained_model!\n","Found 1571 images belonging to 3 classes.\n","Found 329 images belonging to 3 classes.\n","WARNING:tensorflow:From <ipython-input-14-c3acf2b9f89c>:57: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n","Epoch 1/5\n","8/8 [==============================] - ETA: 0s - loss: 0.9010 - acc: 0.7046\n","Epoch 00001: saving model to drive/My Drive/SupportClassification/three_supports/SandV3_modelweights/weights.01-0.95.hdf5.ckpt\n","8/8 [==============================] - 13s 2s/step - loss: 0.9010 - acc: 0.7046 - val_loss: 0.0871 - val_acc: 0.9453\n","Epoch 2/5\n","8/8 [==============================] - ETA: 0s - loss: 0.0613 - acc: 0.9699\n","Epoch 00002: saving model to drive/My Drive/SupportClassification/three_supports/SandV3_modelweights/weights.02-0.98.hdf5.ckpt\n","8/8 [==============================] - 12s 2s/step - loss: 0.0613 - acc: 0.9699 - val_loss: 0.0430 - val_acc: 0.9766\n","Epoch 3/5\n","8/8 [==============================] - ETA: 0s - loss: 0.0616 - acc: 0.9688\n","Epoch 00003: saving model to drive/My Drive/SupportClassification/three_supports/SandV3_modelweights/weights.03-0.88.hdf5.ckpt\n","8/8 [==============================] - 13s 2s/step - loss: 0.0616 - acc: 0.9688 - val_loss: 0.2320 - val_acc: 0.8828\n","Epoch 4/5\n","8/8 [==============================] - ETA: 0s - loss: 0.1135 - acc: 0.9398\n","Epoch 00004: saving model to drive/My Drive/SupportClassification/three_supports/SandV3_modelweights/weights.04-0.98.hdf5.ckpt\n","8/8 [==============================] - 14s 2s/step - loss: 0.1135 - acc: 0.9398 - val_loss: 0.0475 - val_acc: 0.9844\n","Epoch 5/5\n","8/8 [==============================] - ETA: 0s - loss: 0.0296 - acc: 0.9795\n","Epoch 00005: saving model to drive/My Drive/SupportClassification/three_supports/SandV3_modelweights/weights.05-0.98.hdf5.ckpt\n","8/8 [==============================] - 13s 2s/step - loss: 0.0296 - acc: 0.9795 - val_loss: 0.0598 - val_acc: 0.9844\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xw9kbFzr1pNZ","colab_type":"text"},"source":["###Lets load one of the models that we created and see if we can interpret."]},{"cell_type":"code","metadata":{"id":"NW5bo1gf1K0a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"1dbdf5a7-d4c4-48a0-85a4-ee3a7e51022a"},"source":["loaded_model = create_model()\n","loaded_model.load_weights(\"drive/My Drive/SupportClassification/three_supports/SandV3_modelweights/weights.04-0.99.hdf5.ckpt\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-06-04 06:01:54--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.123.128, 2607:f8b0:400c:c15::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.123.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 87910968 (84M) [application/x-hdf]\n","Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n","\n","\r          /tmp/ince   0%[                    ]       0  --.-KB/s               \r         /tmp/incep  68%[============>       ]  57.05M   285MB/s               \r/tmp/inception_v3_w 100%[===================>]  83.84M   297MB/s    in 0.3s    \n","\n","2020-06-04 06:01:55 (297 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fr38psqHyPmJ","colab_type":"text"},"source":["### Now that we have a specific model, lets start looking into interpretability! Lets start by importing some utility functions, as usual."]},{"cell_type":"code","metadata":{"id":"WHO76VSlyXlO","colab_type":"code","colab":{}},"source":["#this can be finicky sometimes... double check next time that theres no problems\n","#sometimes need to reupload net_visualization... and image_utils.py into folder\n","#because they form stupid __pycache__ items\n","import sys\n","sys.path.append('./drive/My Drive/SupportClassification/three_supports')\n","from image_utils import preprocess_image, deprocess_image\n","from net_visualization_tensorflow import compute_saliency_maps"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGs5uUyKWIXF","colab_type":"code","colab":{}},"source":["#this creates a set of training images and training labels\n","sample_training_images, sample_onehot_training_labels = next(train_data_gen) #(128, 150, 150, 3), (128, 2)\n","sample_training_labels = np.zeros((sample_onehot_training_labels.shape[0])) #(128, )\n","for i in range(len(sample_onehot_training_labels)):\n","  sample_training_labels[i] = int(np.where(sample_onehot_training_labels[i] == 1)[0][0])\n","\n","# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n","def plotImages(images_arr):\n","    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n","    axes = axes.flatten()\n","    for img, ax in zip( images_arr, axes):\n","        ax.imshow(img)\n","        ax.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","plotImages(sample_training_images[:5])\n","\n","print(type(sample_training_labels[0]))\n","sample_training_labels = sample_training_labels.astype(np.int64) \n","print(type(sample_training_images[0]))\n","sample_training_images = sample_training_images[:5, :, :, :] #(5, 150, 150, 3), (5,)\n","sample_training_labels = sample_training_labels[:5]\n","print(sample_training_images.shape, sample_training_labels.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5tim-s2mb_w","colab_type":"code","colab":{}},"source":["class_names = {0 : 'NPs', 1 : 'SiO2NBs', 2 : 'TiO2'}\n","\n","def show_saliency_maps(X, y, mask):\n","    mask = np.asarray(mask)\n","    Xm = X[mask]\n","    ym = y[mask]\n","    \n","    saliency = compute_saliency_maps(Xm, ym, loaded_model)\n","\n","    for i in range(mask.size):\n","        plt.subplot(2, mask.size, i + 1)\n","        plt.imshow(deprocess_image(Xm[i]))\n","        plt.axis('off')\n","        plt.title(class_names[ym[i]])\n","        plt.subplot(2, mask.size, mask.size + i + 1)\n","        plt.title(mask[i])\n","        plt.imshow(saliency[i], cmap=plt.cm.hot)\n","        plt.axis('off')\n","        plt.gcf().set_size_inches(20, 8)\n","    plt.show()\n","\n","mask = np.arange(5)\n","show_saliency_maps(sample_training_images, sample_training_labels, mask)"],"execution_count":0,"outputs":[]}]}